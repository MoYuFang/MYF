神经网络都可以规约成一个无重边的有向无环图 $G$，$V$ 是 $G$ 的顶点集合，$E$ 是 $G$ 的边集合，源点集为 $S$，终点只有一个 $t$。

对于任意的 $u\in V$，设 $in(u)$ 是 $u$ 的所有入边的起点的集合（入点集合），$out(u)$ 是 $u$ 所有出边的终点的集合（出点集合），$f_u$ 是关于结点 $u$ 的一个标量。给定 $f$ 的递推公式，当 $u\in S$ 时，$f_u$ 为给定初值 $(f_0)_u$，当 $u\not\in S$ 时 $f_u=f_u(in(u))$。

其中 $f_u(in(u))$ 代表 $f_u$ 关于 $f_{v_1},f_{v_2},...\ (v_1,v_2,...\in in(u))$ 的多元函数。

进一步假设任意 $f_u(in(u))$ 是 $C^1$ 连续函数。

在上面的结构中 $f_t$ 就是其它神经网络中的损失函数的值，源点集合 $S$ 可以分为两个不交子集 $W$ 与 $X$，其中 $(f_0)_w\ (w\in W)$ 就是算法需要学习的参数，$(f_0)_x\ (x\in X)$ 就是神经网络的输入值。



反向传播算法的结果是：对每个 $s\in S$，输出 ${\partial f_t\over \partial f_s}$，算法依据的原理是链式法则。

其中 ${\partial f_t\over \partial f_x}(x\in X)$ 可以用来解释 $f_t$ 对输入值的敏感度，${\partial f_t\over \partial f_w}(w\in W)$ 则指明了神经网络学习时参数增减的方向与幅度。



算法的步骤是：

（1）根据所有初值 $(f_0)_s\ (s\in S)$ 和所有函数 $f_u(in(u))\ (u\not \in S)$ 算出所有标量 $f_u\ (u\in V)$。

（2）使用动态规划中的填表法，用以下递推公式计算对每个结点 $u$ 计算 ${\partial f_t\over \partial f_u}$。
$$
{\partial f_t\over \partial f_u}=\sum_{v\in out(u)}{\partial f_v\over \partial f_u}\cdot {\partial f_t\over \partial f_v}
$$
若单次计算 $f_u(in(u))$ 和 ${\partial f_v\over \partial f_u}\ (v\in out(u))$ 的时间复杂度为 $O(1)$，则反向传播算法的时间复杂度为 $O(|V|+|E|)$。





