
关于计算机视觉，我有个有趣的 idea，它的目标是意念写代码。

想象一下一个科幻场景，你的脑海中想到某个单词，并且有了明确的想写下的念头，然后这个单词就瞬间出现在了笔记本的编辑器上，整个过程不需要用一根手指头敲动一块键盘。

如果这个科幻成为了现实，不但写代码的效率大大提高，体验也会变得极具舒服。很多时候，我们愚笨的手指远远跟不上我们灵敏的大脑，只有很少的勤于 coding 的 coder 才有出神入化的手速，才让代码如德芙般丝滑地从大脑流向编辑器。

这个科幻的目的就是让一个懒于 coding 的 coder，也能通过意念写代码的作弊方式，轻松赶上手速超群的天才 coder 们。

意念写代码之所以是科幻，是因为现有的技术离完全理解人类大脑并捕捉其意念还有很远的一段距离，毕竟连科幻小说《三体》里都写道能对够窥看人类世界一切秘密的三体人来说，唯一的禁地就是人类的大脑。

然而不能理解脑波，不代表意念干预外界就真的不现实。

很明显，能一定程度上反应一个人大脑所思所想的途径有很多，比如他正在干什么就代表他大脑正在想干什么，他十根手指敲下了哪些键就代表了他想要写下什么单词。这两个例子不被认为是意念影响真实物理世界的原因，我认为，从舒适程度的角度来考虑，就是不但消耗体力的累还效率低，根本不是理想化的随心所欲。

但还有其它例子，就是被称作心灵的窗户——眼睛。眼睛只需要轻轻随心转动，视线的焦点就能在大范围的视野内迅速地移动，频率之高、速度之快几乎能实时反应大脑意念的聚焦中心。就意念写代码这个问题而言，瞳孔中心所盯住的那个屏幕上的那一颗像素点，对应的光标位置就是我们想插入代码的位置。

于是乎可以总结出两方面，其一，眼球是外界可见的，可以建模出视线所聚焦的那个点。其二，现有的给大脑贴上若干电极的技术，已经可以实现识别简单逻辑念头。

这意味着只要克服了工程上的重重阻碍，意念写代码这个科幻是可以成为现实的！

为此，我可能可以做到的就是这两方面的第一部分：捕捉视线中心。
大致的可能方案如下：
（1）佩戴一种摄像机眼睛，能在电脑中大致拟合出人眼现在所见的视野。
（2）眼睛上另设置一个不太影响视野的摄像头专门拍摄眼球，然后由计算机视觉这一成熟领域的各种算法，建模出视线的聚焦点。

其实现实中已经有这样的先例存在了，比如全身瘫痪仅剩眼球和一点点面部肌肉可动的霍金先生，工程师们肯定早就整出了一套通过捕捉视线而大致理解霍金先生所思所想的设备。

事实上这一技术被称作眼球追踪。

$\text{KL}$ 散度

设概率函数 $p(x),q(x)$ 的样本空间为 $\Omega$，则 $p$ 相对于 $q$ 的 $\text{KL}$ 散度定义为 
$$
\int_{\Omega}p(x)\log{p(x)\over q(x)}dx
$$
$\text{KL}$ 散度度量了在用 $q(x)$ 去近似 $p(x)$ 时损失的信息。

在传统计算机视觉部分，会有[灰度图](https://www.zhihu.com/search?q=%E7%81%B0%E5%BA%A6%E5%9B%BE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3418597982%7D)、RGB、YUV、均值/高斯滤波器、利用 Canny 算子对图像进行[边缘检测](https://www.zhihu.com/search?q=%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3418597982%7D)、利用[大津算法](https://www.zhihu.com/search?q=%E5%A4%A7%E6%B4%A5%E7%AE%97%E6%B3%95&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3418597982%7D)对图像进行分割等小项目。
[gongchengshi](https://github.com/dongtuoc/cv_learning_resnet50)


### 传统算法

#### 聚类算法

$\text{k-means}$ 算法，用 $\text{KD-Tree}$ 对轮数 $M$ 点数 $N$  数值 $K$ 可以做到约 $O(M(N+K)\log K)$ 的复杂度。

$DBSCAN$ 密度聚类算法，低维时用 $\text{KD-Tree}$ 基本 $O(N\log N)$，高维时用暴力的 $O(N^{2})$。


#### 标注工具

[【数据准备001】标注工具Labelimg安装与使用（附txt与xml文件相互转化代码）](https://blog.csdn.net/m0_46489757/article/details/134973914)

[makesense.ai](https://www.makesense.ai/)
[x-anylabeling](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v1.1.0)

#### Faster-RCNN

转载链接：[http://blog.csdn.net/zy1034092330/article/details/62044941](http://blog.csdn.net/zy1034092330/article/details/62044941)
Faster RCNN github : [https://github.com/rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)  
Faster RCNN paper : [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)

[Faster R-CNN最全讲解](https://blog.csdn.net/weixin_43702653/article/details/124045469)
[目标检测系列——Faster R-CNN原理详解](https://blog.csdn.net/qq_47233366/article/details/125579620)
#### Bottleneck
[Bottleneck层总结](https://blog.csdn.net/weixin_47414581/article/details/139594134)
#### Mobile Net

[MobileNetV1 & MobileNetV2 简介](https://blog.csdn.net/mzpmzk/article/details/82976871)

#### Efficient Net

#### Dense Net

一个 $\text{DenseBlock}$ 中，所有 $w\times h$ 的尺度不变，第 $1,2,..,l-1$ 层的 $\text{BN-ReLU-Conv}$ 前输出，直接与第 $l$ 层的 $\text{BN-ReLU-Conv}$ 前输出进行通道维度 $c$ 上的拼接。

这样 $\text{DenseBlock}$ 的层数不能太多。 

[DenseNet PyTorch 实现](https://github.com/pytorch/vision/blob/main/torchvision/models/densenet.py)

#### CSP Net

基本思想是，将前一阶段的输入特征图在通道 $c$ 这一维分成两部分，一部分经过 $\text{Dense/Res Block}$，另一部分无处理，两部分拼接起来，并通过 $1\times 1$ 的 $\text{Transition}$ 层得到输出特征图。

实际运用中，可改变 $\text{Transition}$ 层的位置

![[CSPNet的三种方式.png]]





#### SPP Net
![[SPP_Net结构.png]]
#### YOLO

[深度学习目标检测：一文弄懂YOLO算法](https://zhuanlan.zhihu.com/p/699522134)
[详解IoU、GIoU、DIoU、CIoU、EIoU和DIoU-NMS](https://blog.csdn.net/qq_41542989/article/details/123846530)
[YOLOv8如何实现卷积？逐行解读ultralytics/nn/modules/conv.py(1)](https://blog.csdn.net/m0_73818664/article/details/139026091)























