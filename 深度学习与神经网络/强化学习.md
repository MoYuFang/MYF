### 无状态强化学习

##### 多臂老虎机问题

有若干个老虎机，第 $a$ 个老虎机下拉时的收益是未知随机变量 $Q(a)\in\{0,1\}$，商家给你 $T$ 次机会，你需要尽可能大的收益。

因为决策不会影响环境，所以适用于无状态强化学习

选取策略：
每次选取，先估计期望上界，然后选取最大的，估计方法如下

##### 霍夫丁不等式
设独立随机变量 $a_{i}\le X_{i}\le b_{i}$，记 $S_{n}=\sum\limits_{i=1}^{n}X_{i}$，则对 $t$ 有不等式成立
$$
P(S_{n}-E(S_{n})\ge t)\le \exp(-{t^{2}\over \sum\limits_{i=1}^{n}(b_{i}-a_{i})^{2}})
$$

设 $S(a),N(a)$ 分别是目前决策 $a$ 的累积收益和选取次数
$$
P\left({S(a)\over N(a)}-E[Q(a)]>\varepsilon\right)\le e^{-2N(a)\varepsilon^2}
$$
取左项为 $p$ 则
$$
\varepsilon\ge\sqrt{-\ln p\over 2N(a)}
$$
当 $p$ 足够小时， $\varepsilon+E[Q(a)]$ 可作为 $Q(a)$ 的期望上界估计。

### 深度强化学习

#### Q-Learn、SARSA

适用于状态离散、行动离散的情况。

记 $Q(s,a)$ 为状态 $s$ 时，进行行动 $a$ 所拥有的最终期望收益，$r(s,a)$ 是立即收益，$\gamma\in(0,1)$ 是衰减因子，则按以下贝尔曼公式迭代
$$
Q(s,a) := Q(s,a)+\gamma\left(r(s,a)+\max_{a'}\set{Q(s',a')}-Q(s,a)\right)
$$
其中 $s'$ 时 agent 交互后到达的新状态。

#### **DQN**

适用于状态连续、行动离散的情况（行动连续时，可插值）

DQN是基于Q学习的，此外，还有基于策略梯度及基于探索与监督的深度强化学习。

训练时的目标是让贝尔曼等式成立 $Q(s,a)=r(s,a)+max_{a'}\set{Q(s',a')}$，损失函数可以用均方差，训练时有以下几点可优化的地方

（1）经验回放：每次交互所获信息是一个 `state, action, reward, next_state` 四元组，可以将最近的若干个（比如10000个）保留，每次训练时就从回放表中随机抽取批次（比如一批64个）。

（2）目标网络：因为 $Q$ 的拟合目标与自身相关，如果每次训练都更新参数的话，训练不稳定，可以增加一个结构相同但参数落后的网络 $Q_{w'}$，然后用 $Q(s,a)$ 拟合 $r(s,a)+max_{s'}\set{Q_{w'}(s',a)}$，训练若干次（如 10 次）后更新 $Q_{w'}$。

（3）$\varepsilon\text{-Greedy}$ 策略：开始训练时，$Q(s,a)$ 与真实值相差较远，$\max_{a'}\set{Q(s',a')}$ 不容易探索新信息，可以设置一个从 1.0 指数衰减且有最小值（如 0.01）的 $\varepsilon$，然后在 agent 交互时有 $\varepsilon$ 的概率进行随机行动，有 $1-\varepsilon$ 的概率选时 $\max_{a'}\set{Q(s',a')} $ 的行动 $a'$。

#### Dueling DQN

有时候状态信息比行为信息重要的多，比如自动驾驶时前方无车这一状态下，汽车向左或向右的影响并不大，修改 DQN 网络结构
$$
Q(s,a)=V(s)+A(s,a)
$$
对整个网络来说，$V$ 与 $A$ 共享一部分参数，因为 $V(s)$ 增加 $C$ 而 $A(s,a)$ 减去 $C$ 时，$Q(s,a)$ 值不变，$V$ 与 $A$ 建模的不稳定性会导致训练的不稳定性，可以将训练目标改成
$$
Q(s,a)=V(s)+A(s,a)-max_{a'}\set{A(s, a')}
$$
或
$$
Q(s,a)=V(s)+A(s,a)-{1\over |A|}\set{A(s, a')}
$$
后者在实际中更稳定。

#### 策略梯度学习

将 agent 与环境的交互看作马尔科夫决策过程，即假定一个状态转移的概率函数 $p(s'|s,a)$ 代表状态为 $s$ 且行动为 $a$ 时，转移到状态 $s'$ 的概率，agent 的策略也假定为 $\pi(a|s)$ 代表状态为 $s$ 时 agent 选择行动 $a$ 的概率。一般而言 $p$ 函数是未知的、环境交互（采样）间接获取信息的，$\pi$ 函数则是 agent 通过训练而优化的。

沿用 Q-Learn 思想，定义两个价值函数 $Q^{\pi}(s,a)$ 和 $V^{\pi}(s)$，则有贝尔曼期望等式
$$
Q^{\pi}(s,a)=r(s,a)+\gamma\sum_{s'}p(s'|s,a)V^{\pi}(s')\\
V^{\pi}(s)=\sum_{a}\pi(a|s)Q^{\pi}(s,a)=\sum_{a}\pi(a|s)\left(r(s,a)+\gamma\sum_{s'}p(s'|s,a)V^{\pi}(s')\right)
$$
策略梯度学习意味着，通过学习参数 $\theta$ 得到 $\pi_{\theta}(a|s)$，目标是最大化初始状态 $s_{0}$ 的 $V^{\pi_{\theta}}(s_{0})$。

考虑其梯度 $\nabla_{\theta}V^{\pi_{\theta}}(s)$
$$
\begin{aligned}
\nabla_{\theta}V^{\pi_{\theta}}(s)&=\nabla_{\theta}\left(\sum_{a}\pi(a|s)(r(s,a)+\gamma\sum_{s'}p(s'|s,a)V^{\pi}(s'))\right)\\
&=\sum_{a}Q^{\pi}(s,a)\nabla_{\theta}\pi(a|s)+\gamma\sum_{a}\pi(a|s)\sum_{s'}p(s'|s,a)\nabla_{\theta}V^{\pi}(s')
\end{aligned}
$$
记 $\displaystyle \Phi(s)=\sum_{a}Q^{\pi}(s,a)\nabla_{\theta}\pi(a|s)$，$d(s,x,k)$ 代表在策略 $\pi_{\theta}$ 下从状态 $s$ 走 $k$ 步到 $x$ 的概率，则
$$
\begin{aligned}
\nabla_{\theta}V^{\pi_{\theta}}(s)&=\Phi(s)+\gamma \sum_{s'}d(s,s',1)\nabla_{\theta}V^{\pi}(s')\\
&=\Phi(s)+\gamma\sum_{s'}d(s,s',1)\Phi(s')+\gamma^{2}\sum_{s''}d(s,s'',2)\Phi(s'')+...\\
&=\sum_{k=0}^{\infty}\gamma^{k}\sum_{s'}\Phi(s')d(s,s',k)
\end{aligned}
$$
其中 $d(s,s',0)=\left\{\begin{matrix}1, s=s'\\0,s\neq s'\end{matrix}\right.$，记 $\displaystyle N(s)=\sum_{k=0}^{\infty}\gamma^{k}d(s_0 ,s,k)$，则
$$
\begin{aligned}
\nabla_{\theta}V^{\pi_{\theta}}(s_{0})&=\sum_{s}N(s)\Phi(s)\\
&=\sum_{s'}N(s')\cdot\sum_{s}{N(s)\over\sum_{s'}N(s')}\Phi(s)\\
&={1\over 1-\gamma}\sum_{s}\nu^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(a|s)Q^{\pi_{\theta}}(s,a){\nabla_{\theta}\pi_{\theta}(a|s)\over\pi_{\theta}(a|s)}
\end{aligned}
$$
固定 $s$，考虑 
$$
\nu^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(a|s)Q^{\pi_{\theta}}(s,a)
$$
究竟意味着什么？它的含义是以 $s_0$ 为起点时 $s$ 对 $V^{\pi_{\theta}}(s_0)$ 的贡献，于是可以用蒙特卡洛来估计它，具体的

假设 agent 从 $s_0$ 开始按 $\pi_{\theta}$ 策略交互 $T$ 步得到的状态序列 $s_{i}(1\le i\le T)$、奖励序列 $r_{i}(0\le i< T)$ 和行动序列 $a_i(0\le i< T)$，则状态 $s_i$ 对 $V^{\pi_{\theta}}(s_0)$ 的贡献可估计为
$$
\sum_{t=i}^{T}\gamma^{t-i}r_{i}
$$
注意序列允许 $\set{s_i}$ 中有重复状态，但不影响总贡献，于是有以下估计
$$
\nabla_{\theta}V^{\pi_{\theta}}(s_{0})\approx\sum_{i=0}^{T}\sum_{t=i}^{T}\gamma^{t-i}r_i\nabla_{\theta}(\ln\pi_{\theta}(a_i|s_i))
$$
于是一次 $T$ 步环境交互对网络参数的更新可以通过以下伪代码完成

```
G = 0
for i in {T,T-1,T-2,...,0}:
	G = gamma * G + r[i]
	loss = -log_prob[i] * G
	loss.backward() #更新梯度
optimizer.step() #更新参数
```

#### Actor-Critic

策略梯度法的实质是用蒙特卡洛法估计
$$
\psi(s)=\nu^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(a|s)Q^{\pi_{\theta}}(s,a)
$$
那还有其它方法估计吗？有的，$\psi(s)$ 可以有很多形式，Actor-Critic 中用的是**时序差分残差**
$$
\psi(s_{t})=r_{t}+\gamma V(s_{t+1})-V(s_{t})
$$
Actor-Critic 中有两个模型 Actor 和 Critic，Actor 拟合策略函数 $\pi(a|s)$ ，使用时序差分残差版的策略梯度法下降，Critic 拟合价值函数 $V(s)$，使用均方差，伪代码是：

- 初始化 Actor 参数 $\theta$，Critic 参数 $\omega$

- **for** 循环 **epoches** 次

  - 根据策略 $\pi_{\theta}$ 采样

  - 计算 $\set{\delta_{t}=r_{t}+\gamma V_{\omega}(s_{t+1})-V_{\omega}(s_{t})}$

  - 更新 $\omega:=\omega+\alpha_{\omega}\sum_{t}\delta_{t}\nabla_{\omega}V_{\omega}(s_{t})$

  - 更新 $\theta := \theta+\alpha_{\theta}\sum_{t}\delta_{t}\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})$

